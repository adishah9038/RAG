{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Os, Sync, Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv_path=\"../.env\")\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "azure_base_url = os.environ[\"AZURE_BASE_URL\"]\n",
    "# From model URL endpoint\n",
    "azure_api_version = \"2024-08-01-preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LlamaIndexLLMWrapper\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "evaluator_llm = LlamaIndexLLMWrapper(AzureOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    deployment_name=\"gpt-4o-mini\",\n",
    "    api_key=azure_api_key,\n",
    "    azure_endpoint=azure_base_url,\n",
    "    api_version=azure_api_version,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROQ\n",
    "QROQ_API_KEY = os.environ['GROQ_API_KEY_1']\n",
    "from llama_index.llms.groq import Groq\n",
    "groq_llm = Groq(model=\"llama3-8b-8192\", api_key = QROQ_API_KEY, set_run_config=None)\n",
    "print(\"Groq: \"+str(groq_llm.complete(\"Hi\")))\n",
    "# Cohere\n",
    "CO_API_KEY = os.environ['COHERE_API_KEY']\n",
    "from llama_index.llms.cohere import Cohere\n",
    "cohere_llm = Cohere(model=\"command-light\",api_key=CO_API_KEY)\n",
    "print(\"Cohere: \"+str(cohere_llm.complete(\"Hi\")))\n",
    "# Fast Embed\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "fast_embed = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "from llama_index.core.settings import Settings\n",
    "Settings.llm = groq_llm\n",
    "Settings.embed_model = fast_embed\n",
    "\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "docstore = SimpleDocumentStore.from_persist_dir(persist_dir=\"D:/Learning New/GenAI/Project_RAG/RAG/Data_cleaning/doc_store\")\n",
    "documents = list(docstore.docs.values())\n",
    "\n",
    "from llama_index.core import Document, VectorStoreIndex\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    # remember, you must pass a list of documents!\n",
    "    documents, \n",
    "    embed_model=Settings.embed_model,\n",
    "    show_progress=True)\n",
    "\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core import PromptTemplate\n",
    "# from prompts import QUESTION_GEN_PROMPT\n",
    "\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(docstore=index.docstore, similarity_top_k=10)\n",
    "\n",
    "# QUERY_GEN_PROMPT_TEMPLATE=PromptTemplate(QUESTION_GEN_PROMPT)\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=10,\n",
    "    num_queries=1,  # set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    "    # query_gen_prompt=QUERY_GEN_PROMPT_TEMPLATE, \n",
    ")\n",
    "\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "similarity_processor = SimilarityPostprocessor(similarity_cutoff=0.025)\n",
    "\n",
    "response_synthsizer = get_response_synthesizer(llm=Settings.llm)\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthsizer,\n",
    "    node_postprocessors=[similarity_processor],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import Faithfulness, SemanticSimilarity, FactualCorrectness, LLMContextRecall\n",
    "from ragas.integrations.llama_index import evaluate\n",
    "from datasets import load_dataset\n",
    "\n",
    "from ragas import EvaluationDataset\n",
    "eval_dataset = EvaluationDataset.from_pandas(df_dropped)\n",
    "evaluator_llm = generator_llm\n",
    "evaluator_embeddings = generator_embeddings\n",
    "metrics = [\n",
    "    LLMContextRecall(llm=evaluator_llm)\n",
    "]\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics, batch_size=2, query_engine=query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
