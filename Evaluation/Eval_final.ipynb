{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Os, Sync, Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv_path=\"../.env\")\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "azure_base_url = os.environ[\"AZURE_BASE_URL\"]\n",
    "# From model URL endpoint\n",
    "azure_api_version = \"2024-08-01-preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM & EMBEDDING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq: Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "Cohere:  Hi, it's nice to connect with you! How can I assist you today? I am here to ready to provide answers to connect or explore ideas. Feel free to get your critical information, and I can assist you with many areas of interest, including answering questions and providing creative content ideas. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# GROQ\n",
    "QROQ_API_KEY = os.environ['GROQ_API_KEY_1']\n",
    "from llama_index.llms.groq import Groq\n",
    "groq_llm = Groq(model=\"llama3-8b-8192\", api_key = QROQ_API_KEY, set_run_config=None)\n",
    "print(\"Groq: \"+str(groq_llm.complete(\"Hi\")))\n",
    "# Cohere\n",
    "CO_API_KEY = os.environ['COHERE_API_KEY']\n",
    "from llama_index.llms.cohere import Cohere\n",
    "cohere_llm = Cohere(model=\"command-light\",api_key=CO_API_KEY)\n",
    "print(\"Cohere: \"+str(cohere_llm.complete(\"Hi\")))\n",
    "# Fast Embed\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "fast_embed = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "from ragas.llms import LlamaIndexLLMWrapper\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "evaluator_llm = LlamaIndexLLMWrapper(AzureOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    deployment_name=\"gpt-4o-mini\",\n",
    "    api_key=azure_api_key,\n",
    "    azure_endpoint=azure_base_url,\n",
    "    api_version=azure_api_version,\n",
    "))\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from ragas.embeddings import LlamaIndexEmbeddingsWrapper\n",
    "evaluator_embedding = LlamaIndexEmbeddingsWrapper(AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"text-embedding-ada-002\",\n",
    "    api_key=azure_api_key,\n",
    "    azure_endpoint=azure_base_url,\n",
    "    api_version=\"2023-05-15\",\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  Company Name - Attogram\\n                    Product List / Index - 1mg - Moisture Analyzer - 0.01%, 0.1mg - Moisture Analyzer - 0.005%, 0.01mg Semi Micro / Analytical Balance , 0.1mg - Analytical Balance, 1mg Balances, 10mg Balances, 100mg Balances, Table Top Scales, Crane Scales, Standard Weights, Antivibration Pad, Density Kit\\n              ']\n",
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['retrieved_contexts', 'response'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_contexts\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Check the first value\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_contexts\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]))  \u001b[38;5;66;03m# Inspect the type\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretrieved_contexts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresponse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())  \u001b[38;5;66;03m# Check for missing values\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretrieved_contexts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Inspect the first row\u001b[39;00m\n",
      "File \u001b[1;32md:\\Learning New\\GenAI\\Project_RAG\\RAG\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32md:\\Learning New\\GenAI\\Project_RAG\\RAG\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32md:\\Learning New\\GenAI\\Project_RAG\\RAG\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['retrieved_contexts', 'response'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"D:/Learning New/GenAI/Project_RAG/RAG/1. Data/evaluator1_10.csv\")\n",
    "import ast\n",
    "df['reference_contexts'] = df['reference_contexts'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "print(df['reference_contexts'].iloc[0])  # Check the first value\n",
    "print(type(df['reference_contexts'].iloc[0]))  # Inspect the type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "Settings.llm = groq_llm\n",
    "Settings.embed_model = fast_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Index from Persisted Index Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "storage_context1 = StorageContext.from_defaults(persist_dir=\"D:/Learning New/GenAI/Project_RAG/RAG/1. Data/index_store\")\n",
    "index_from_docs = load_index_from_storage(storage_context1)\n",
    "storage_context2 = StorageContext.from_defaults(persist_dir=\"D:/Learning New/GenAI/Project_RAG/RAG/1. Data/index_from_node_store\")\n",
    "index_from_nodes = load_index_from_storage(storage_context2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Difference - PEB241T and PEB347T. What is capacity for each instrument?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval (Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_from_docs = index_from_docs.as_retriever(\n",
    "    similarity_top_k=10,\n",
    "    similarity_threshold=0.75)\n",
    "retriever_from_nodes = index_from_nodes.as_retriever(\n",
    "    similarity_top_k=10,\n",
    "    similarity_threshold=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Engine (Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# configure a post processor\n",
    "similarity_processor = SimilarityPostprocessor(similarity_cutoff=0.42)\n",
    "\n",
    "# configure a response sythesizer\n",
    "response_synthsizer = get_response_synthesizer(llm=Settings.llm)\n",
    "\n",
    "# create a query engine\n",
    "default_query_engine_from_docs = RetrieverQueryEngine(\n",
    "    retriever=retriever_from_docs,\n",
    "    response_synthesizer=response_synthsizer,\n",
    "    node_postprocessors=[similarity_processor],\n",
    ")\n",
    "# create a query engine\n",
    "default_query_engine_from_nodes = RetrieverQueryEngine(\n",
    "    retriever=retriever_from_nodes,\n",
    "    response_synthesizer=response_synthsizer,\n",
    "    node_postprocessors=[similarity_processor],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying (Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context information, the capacity for PEB241T is 220g and 320g, and for PEB347T, it is not mentioned.\n",
      "The capacity for PEB241T is 220g and 320g, and the capacity for PEB347T is also 220g and 320g.\n"
     ]
    }
   ],
   "source": [
    "# Querying\n",
    "print(default_query_engine_from_docs.query(query).response)\n",
    "print(default_query_engine_from_nodes.query(query).response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Default Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Query Engine: 100%|██████████| 10/10 [00:09<00:00,  1.01it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [01:06<00:00,  6.66s/it]\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import Faithfulness, LLMContextRecall, AnswerRelevancy\n",
    "from ragas.integrations.llama_index import evaluate\n",
    "from datasets import load_dataset\n",
    "from ragas import EvaluationDataset\n",
    "eval_dataset = EvaluationDataset.from_pandas(df)\n",
    "metrics = [\n",
    "    LLMContextRecall(llm=evaluator_llm)\n",
    "]\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics, batch_size=1, query_engine=default_query_engine_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_recall': 0.4333}\n",
      "retrieved_contexts    0\n",
      "response              0\n",
      "dtype: int64\n",
      "retrieved_contexts    [Company Name - Attogram\\n                    ...\n",
      "response              The Semi Micro category includes the following...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "df_new = results.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Query Engine: 100%|██████████| 10/10 [00:09<00:00,  1.04it/s]\n",
      "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]Exception raised in Job[8]: AuthenticationError(Error code: 401 - {'error': {'message': 'Incorrect API key provided: 9nTwVLK8************************************************************************mi9K. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}})\n",
      "Evaluating:  10%|█         | 1/10 [02:16<20:30, 136.67s/it]Exception raised in Job[6]: AuthenticationError(Error code: 401 - {'error': {'message': 'Incorrect API key provided: 9nTwVLK8************************************************************************mi9K. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}})\n",
      "Exception raised in Job[4]: AuthenticationError(Error code: 401 - {'error': {'message': 'Incorrect API key provided: 9nTwVLK8************************************************************************mi9K. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}})\n",
      "Exception raised in Job[1]: AuthenticationError(Error code: 401 - {'error': {'message': 'Incorrect API key provided: 9nTwVLK8************************************************************************mi9K. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}})\n",
      "Exception raised in Job[0]: AuthenticationError(Error code: 401 - {'error': {'message': 'Incorrect API key provided: 9nTwVLK8************************************************************************mi9K. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}})\n",
      "Exception raised in Job[2]: AuthenticationError(Error code: 401 - {'error': {'message': 'Incorrect API key provided: 9nTwVLK8************************************************************************mi9K. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}})\n",
      "Exception raised in Job[7]: AuthenticationError(Error code: 401 - {'error': {'message': 'Incorrect API key provided: 9nTwVLK8************************************************************************mi9K. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}})\n",
      "Evaluating:  20%|██        | 2/10 [02:18<07:39, 57.46s/it] Exception raised in Job[5]: AuthenticationError(Error code: 401 - {'error': {'message': 'Incorrect API key provided: 9nTwVLK8************************************************************************mi9K. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}})\n",
      "Evaluating:  80%|████████  | 8/10 [02:29<00:19,  9.56s/it]Exception raised in Job[3]: TimeoutError()\n",
      "Evaluating:  90%|█████████ | 9/10 [03:00<00:14, 14.77s/it]Exception raised in Job[9]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 10/10 [03:00<00:00, 18.00s/it]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m EvaluationDataset\u001b[38;5;241m.\u001b[39mfrom_pandas(df)\n\u001b[0;32m      8\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     AnswerRelevancy(llm\u001b[38;5;241m=\u001b[39mevaluator_llm)\n\u001b[0;32m     10\u001b[0m ]\n\u001b[1;32m---> 11\u001b[0m results_1 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_query_engine_from_docs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Learning New\\GenAI\\Project_RAG\\RAG\\.venv\\Lib\\site-packages\\ragas\\integrations\\llama_index.py:91\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(query_engine, dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, batch_size, token_usage_parser, raise_exceptions, column_map, show_progress)\u001b[0m\n\u001b[0;32m     88\u001b[0m     sample\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m responses[i]\n\u001b[0;32m     89\u001b[0m     sample\u001b[38;5;241m.\u001b[39mretrieved_contexts \u001b[38;5;241m=\u001b[39m retrieved_contexts[i]\n\u001b[1;32m---> 91\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mragas_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mli_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mli_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mRunConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_ci\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_ci\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_usage_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_usage_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32md:\\Learning New\\GenAI\\Project_RAG\\RAG\\.venv\\Lib\\site-packages\\ragas\\_analytics.py:205\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m    204\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m--> 205\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\Learning New\\GenAI\\Project_RAG\\RAG\\.venv\\Lib\\site-packages\\ragas\\evaluation.py:333\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;66;03m# evalution run was successful\u001b[39;00m\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;66;03m# now lets process the results\u001b[39;00m\n\u001b[0;32m    332\u001b[0m     cost_cb \u001b[38;5;241m=\u001b[39m ragas_callbacks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcost_cb\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcost_cb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ragas_callbacks \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mEvaluationResult\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbinary_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcost_cb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m            \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUnion\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCostCallbackHandler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcost_cb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mragas_traces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[0;32m    345\u001b[0m         evaluation_rm\u001b[38;5;241m.\u001b[39mon_chain_end({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\u001b[38;5;241m.\u001b[39mscores})\n",
      "File \u001b[1;32m<string>:10\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, scores, dataset, binary_columns, cost_cb, traces, ragas_traces, run_id)\u001b[0m\n",
      "File \u001b[1;32md:\\Learning New\\GenAI\\Project_RAG\\RAG\\.venv\\Lib\\site-packages\\ragas\\dataset_schema.py:410\u001b[0m, in \u001b[0;36mEvaluationResult.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# parse the traces\u001b[39;00m\n\u001b[0;32m    409\u001b[0m run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_id) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 410\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraces \u001b[38;5;241m=\u001b[39m \u001b[43mparse_run_traces\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mragas_traces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Learning New\\GenAI\\Project_RAG\\RAG\\.venv\\Lib\\site-packages\\ragas\\callbacks.py:167\u001b[0m, in \u001b[0;36mparse_run_traces\u001b[1;34m(traces, parent_run_id)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, prompt_uuid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(metric_trace\u001b[38;5;241m.\u001b[39mchildren):\n\u001b[0;32m    164\u001b[0m         prompt_trace \u001b[38;5;241m=\u001b[39m traces[prompt_uuid]\n\u001b[0;32m    165\u001b[0m         prompt_traces[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_trace\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    166\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_trace\u001b[38;5;241m.\u001b[39minputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[1;32m--> 167\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mprompt_trace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m    168\u001b[0m         }\n\u001b[0;32m    169\u001b[0m     metric_traces[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_trace\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt_traces\n\u001b[0;32m    170\u001b[0m parased_traces\u001b[38;5;241m.\u001b[39mappend(metric_traces)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import Faithfulness, LLMContextRecall, AnswerRelevancy\n",
    "from ragas.integrations.llama_index import evaluate\n",
    "from datasets import load_dataset\n",
    "\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "eval_dataset = EvaluationDataset.from_pandas(df)\n",
    "metrics = [\n",
    "    AnswerRelevancy(llm=evaluator_llm)\n",
    "]\n",
    "results_1 = evaluate(dataset=eval_dataset, metrics=metrics, batch_size=1, query_engine=default_query_engine_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults_1\u001b[49m)\n\u001b[0;32m      2\u001b[0m results_1\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results_1' is not defined"
     ]
    }
   ],
   "source": [
    "print(results_1)\n",
    "results_1.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_1)\n",
    "df_2 = results_1.to_pandas()\n",
    "from ragas.metrics import Faithfulness, LLMContextRecall, AnswerRelevancy\n",
    "from ragas.integrations.llama_index import evaluate\n",
    "from datasets import load_dataset\n",
    "\n",
    "from ragas import EvaluationDataset\n",
    "eval_dataset = EvaluationDataset.from_pandas(df_2)\n",
    "metrics = [\n",
    "    Faithfulness(llm=evaluator_llm)\n",
    "]\n",
    "results_2 = evaluate(dataset=eval_dataset, metrics=metrics, batch_size=1, query_engine=default_query_engine_from_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval (BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n"
     ]
    }
   ],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "bm25_retriever_from_nodes = BM25Retriever.from_defaults(docstore=index_from_nodes.docstore, similarity_top_k=10)\n",
    "bm25_retriever_from_docs = BM25Retriever.from_defaults(docstore=index_from_docs.docstore, similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Engine (BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# configure a post processor\n",
    "similarity_processor = SimilarityPostprocessor(similarity_cutoff=0.42)\n",
    "\n",
    "# configure a response sythesizer\n",
    "response_synthsizer = get_response_synthesizer(llm=Settings.llm)\n",
    "\n",
    "# create a query engine\n",
    "bm25_query_engine_from_docs = RetrieverQueryEngine(\n",
    "    retriever=bm25_retriever_from_docs,\n",
    "    response_synthesizer=response_synthsizer,\n",
    "    node_postprocessors=[similarity_processor],\n",
    ")\n",
    "# create a query engine\n",
    "bm25_query_engine_from_nodes = RetrieverQueryEngine(\n",
    "    retriever=bm25_retriever_from_nodes,\n",
    "    response_synthesizer=response_synthsizer,\n",
    "    node_postprocessors=[similarity_processor],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying (BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capacity for PEB241T is 220g and the capacity for PEB347T is 320g.\n",
      "The capacity for PEB241T is 220g and 320g, and for PEB347T it is also 220g and 320g.\n"
     ]
    }
   ],
   "source": [
    "# Querying\n",
    "print(bm25_query_engine_from_docs.query(query).response)\n",
    "print(bm25_query_engine_from_nodes.query(query).response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval (Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "hybrid_retriever_from_docs = QueryFusionRetriever(\n",
    "    [bm25_query_engine_from_docs, retriever_from_docs],\n",
    "    similarity_top_k=10,\n",
    "    num_queries=1,\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    ")\n",
    "hybrid_retriever_from_nodes = QueryFusionRetriever(\n",
    "    [bm25_query_engine_from_nodes, retriever_from_nodes],\n",
    "    similarity_top_k=10,\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    num_queries=1,\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Engine (Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# configure a post processor\n",
    "similarity_processor = SimilarityPostprocessor(similarity_cutoff=0.02)\n",
    "\n",
    "# configure a response sythesizer\n",
    "response_synthsizer = get_response_synthesizer(llm=Settings.llm)\n",
    "\n",
    "# create a query engine\n",
    "hybrid_query_engine_from_docs = RetrieverQueryEngine(\n",
    "    retriever=hybrid_retriever_from_docs,\n",
    "    response_synthesizer=response_synthsizer,\n",
    "    node_postprocessors=[similarity_processor],\n",
    ")\n",
    "# create a query engine\n",
    "hybrid_query_engine_from_nodes = RetrieverQueryEngine(\n",
    "    retriever=hybrid_retriever_from_nodes,\n",
    "    response_synthesizer=response_synthsizer,\n",
    "    node_postprocessors=[similarity_processor],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying (Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capacity for PEB241T is 220g and the capacity for PEB347T is 320g.\n",
      "The capacity for PEB241T is 220g and the capacity for PEB347T is 320g.\n"
     ]
    }
   ],
   "source": [
    "# Querying\n",
    "print(hybrid_query_engine_from_docs.query(query).response)\n",
    "print(hybrid_query_engine_from_nodes.query(query).response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
